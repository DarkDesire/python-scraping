{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printCounter(counter, min_threshold = 10):\n",
    "    print(newdict = {x: count for x, count in sorted(getNgrams(content, 2).items(), key=lambda item: -item[1]) if count >= min_threshold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def cleanSentence(sentence):\n",
    "    sentence = sentence.split(' ')\n",
    "    sentence = [word.strip(string.punctuation+string.whitespace) for word in sentence]\n",
    "    sentence = [word for word in sentence if len(word) > 1 or (word.lower() == 'a' or word.lower() == 'i')]\n",
    "    return sentence\n",
    "\n",
    "def cleanInput(content):\n",
    "    content = content.upper()\n",
    "    content = re.sub('\\n', ' ', content)\n",
    "    content = bytes(content, 'UTF-8')\n",
    "    content = content.decode('ascii', 'ignore')\n",
    "    sentences = content.split('. ')\n",
    "    return [cleanSentence(sentence) for sentence in sentences]\n",
    "\n",
    "def getNgramsFromSentence(content, n):\n",
    "    output = []\n",
    "    for i in range(len(content)-n+1):\n",
    "        output.append(content[i:i+n])\n",
    "    return output\n",
    "\n",
    "def getNgrams(content, n):\n",
    "    content = cleanInput(content)\n",
    "    ngrams = Counter()\n",
    "    ngrams_list = []\n",
    "    for sentence in content:\n",
    "        newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, n)]\n",
    "        ngrams_list.extend(newNgrams)\n",
    "        ngrams.update(newNgrams)\n",
    "    return(ngrams)\n",
    "\n",
    "\n",
    "content = str(\n",
    "      urlopen('http://pythonscraping.com/files/inaugurationSpeech.txt').read(),\n",
    "              'utf-8')\n",
    "ngrams = getNgrams(content, 3)\n",
    "printCounter(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def isCommon(ngram):\n",
    "    commonWords = ['THE', 'BE', 'AND', 'OF', 'A', 'IN', 'TO', 'HAVE', 'IT', 'I', 'THAT', 'FOR', 'YOU', 'HE', 'WITH', 'ON', 'DO', 'SAY', 'THIS', 'THEY', 'IS', 'AN', 'AT', 'BUT', 'WE', 'HIS', 'FROM', 'THAT', 'NOT', 'BY', 'SHE', 'OR', 'AS', 'WHAT', 'GO', 'THEIR', 'CAN', 'WHO', 'GET', 'IF', 'WOULD', 'HER', 'ALL', 'MY', 'MAKE', 'ABOUT', 'KNOW', 'WILL', 'AS', 'UP', 'ONE', 'TIME', 'HAS', 'BEEN', 'THERE', 'YEAR', 'SO', 'THINK', 'WHEN', 'WHICH', 'THEM', 'SOME', 'ME', 'PEOPLE', 'TAKE', 'OUT', 'INTO', 'JUST', 'SEE', 'HIM', 'YOUR', 'COME', 'COULD', 'NOW', 'THAN', 'LIKE', 'OTHER', 'HOW', 'THEN', 'ITS', 'OUR', 'TWO', 'MORE', 'THESE', 'WANT', 'WAY', 'LOOK', 'FIRST', 'ALSO', 'NEW', 'BECAUSE', 'DAY', 'MORE', 'USE', 'NO', 'MAN', 'FIND', 'HERE', 'THING', 'GIVE', 'MANY', 'WELL']\n",
    "    for word in ngram:\n",
    "        if word in commonWords:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def getNgramsFromSentence(content, n):\n",
    "    output = []\n",
    "    for i in range(len(content)-n+1):\n",
    "        if not isCommon(content[i:i+n]):\n",
    "            output.append(content[i:i+n])\n",
    "    return output\n",
    "\n",
    "ngrams = getNgrams(content, 3)\n",
    "print(ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFirstSentenceContaining(ngram, content):\n",
    "    #print(ngram)\n",
    "    sentences = content.upper().split(\". \")\n",
    "    for sentence in sentences: \n",
    "        if ngram in sentence:\n",
    "            return sentence+'\\n'\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "print(getFirstSentenceContaining('EXCLUSIVE METALLIC CURRENCY', content))\n",
    "print(getFirstSentenceContaining('EXECUTIVE DEPARTMENT', content))\n",
    "print(getFirstSentenceContaining('GENERAL GOVERNMENT', content))\n",
    "print(getFirstSentenceContaining('CALLED UPON', content))\n",
    "print(getFirstSentenceContaining('CHIEF MAGISTRATE', content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from random import randint\n",
    "\n",
    "def wordListSum(wordList):\n",
    "    sum = 0\n",
    "    for word, value in wordList.items():\n",
    "        sum += value\n",
    "    return sum\n",
    "\n",
    "def retrieveRandomWord(wordList):\n",
    "    randIndex = randint(1, wordListSum(wordList))\n",
    "    for word, value in wordList.items():\n",
    "        randIndex -= value\n",
    "        if randIndex <= 0:\n",
    "            return word\n",
    "\n",
    "def buildWordDict(text):\n",
    "    # Remove newlines and quotes\n",
    "    text = text.replace('\\n', ' ');\n",
    "    text = text.replace('\"', '');\n",
    "\n",
    "    # Make sure punctuation marks are treated as their own \"words,\"\n",
    "    # so that they will be included in the Markov chain\n",
    "    punctuation = [',','.',';',':']\n",
    "    for symbol in punctuation:\n",
    "        text = text.replace(symbol, ' {} '.format(symbol));\n",
    "\n",
    "    words = text.split(' ')\n",
    "    # Filter out empty words\n",
    "    words = [word for word in words if word != '']\n",
    "\n",
    "    wordDict = {}\n",
    "    for i in range(1, len(words)):\n",
    "        if words[i-1] not in wordDict:\n",
    "                # Create a new dictionary for this word\n",
    "            wordDict[words[i-1]] = {}\n",
    "        if words[i] not in wordDict[words[i-1]]:\n",
    "            wordDict[words[i-1]][words[i]] = 0\n",
    "        wordDict[words[i-1]][words[i]] += 1\n",
    "    return wordDict\n",
    "\n",
    "text = str(urlopen('http://pythonscraping.com/files/inaugurationSpeech.txt')\n",
    "          .read(), 'utf-8')\n",
    "wordDict = buildWordDict(text)\n",
    "\n",
    "#Generate a Markov chain of length 100\n",
    "length = 100\n",
    "chain = ['I']\n",
    "for i in range(0, length):\n",
    "    newWord = retrieveRandomWord(wordDict[chain[-1]])\n",
    "    chain.append(newWord)\n",
    "\n",
    "print(' '.join(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock', user='root', passwd='root', db='mysql', charset='utf8')\n",
    "cur = conn.cursor()\n",
    "cur.execute('USE wikipedia')\n",
    "\n",
    "def getUrl(pageId):\n",
    "    cur.execute('SELECT url FROM pages WHERE id = %s', (int(pageId)))\n",
    "    return cur.fetchone()[0]\n",
    "\n",
    "def getLinks(fromPageId):\n",
    "    cur.execute('SELECT toPageId FROM links WHERE fromPageId = %s', (int(fromPageId)))\n",
    "    if cur.rowcount == 0:\n",
    "        return []\n",
    "    return [x[0] for x in cur.fetchall()]\n",
    "\n",
    "def searchBreadth(targetPageId, paths=[[1]]):\n",
    "    newPaths = []\n",
    "    for path in paths:\n",
    "        links = getLinks(path[-1])\n",
    "        for link in links:\n",
    "            if link == targetPageId:\n",
    "                return path + [link]\n",
    "            else:\n",
    "                newPaths.append(path+[link])\n",
    "    return searchBreadth(targetPageId, newPaths)\n",
    "                \n",
    "nodes = getLinks(1)\n",
    "targetPageId = 28624\n",
    "pageIds = searchBreadth(targetPageId)\n",
    "for pageId in pageIds:\n",
    "    print(getUrl(pageId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
